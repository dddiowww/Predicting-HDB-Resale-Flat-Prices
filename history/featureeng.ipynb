{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "253e36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 15)\n",
      "测试集形状: (50000, 20)\n",
      "\n",
      "训练集前5行:\n",
      "    MONTH  YEAR  MONTH_INT\n",
      "0  Oct-20  2020         10\n",
      "1  Jul-21  2021          7\n",
      "2  May-21  2021          5\n",
      "3  Aug-21  2021          8\n",
      "4  May-23  2023          5\n",
      "\n",
      "测试集前5行:\n",
      "     MONTH  YEAR  MONTH_INT\n",
      "0  2025-06  2025          6\n",
      "1  2020-01  2020          1\n",
      "2  2025-06  2025          6\n",
      "3  2022-10  2022         10\n",
      "4  2024-02  2024          2\n"
     ]
    }
   ],
   "source": [
    "# 将MONTH列拆分为年份和月份两列\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取训练集和测试集\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 从MONTH列中提取年份和月份\n",
    "# MONTH格式为 \"Oct-20\" 或 \"Jul-21\"，需要解析月份缩写和2位年份\n",
    "month_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "def extract_year_month(month_str):\n",
    "    \"\"\"\n",
    "    从 MONTH 列中提取年份和月份\n",
    "    支持两种格式：\n",
    "    1. 'Oct-20' 或 'Jul-21'（月份缩写-2位年份）\n",
    "    2. '2025-06' 或 '2020-01'（YYYY-MM格式）\n",
    "    \"\"\"\n",
    "    parts = month_str.split('-')\n",
    "    \n",
    "    # 检查是否为 YYYY-MM 格式（第一部分是4位数字）\n",
    "    if len(parts[0]) == 4 and parts[0].isdigit():\n",
    "        # 格式：2025-06\n",
    "        year = int(parts[0])\n",
    "        month = int(parts[1])\n",
    "    else:\n",
    "        # 格式：Oct-20 或 Jul-21\n",
    "        month_abbr = parts[0]\n",
    "        year_2digit = int(parts[1])\n",
    "        \n",
    "        # 将2位年份转换为4位年份（假设20-99表示2000-2099）\n",
    "        year = 2000 + year_2digit if year_2digit < 100 else year_2digit\n",
    "        month = month_map.get(month_abbr, 0)\n",
    "    \n",
    "    return year, month\n",
    "\n",
    "# 应用函数提取年份和月份\n",
    "df_train[['YEAR', 'MONTH_INT']] = df_train['MONTH'].apply(\n",
    "    lambda x: pd.Series(extract_year_month(x))\n",
    ")\n",
    "df_test[['YEAR', 'MONTH_INT']] = df_test['MONTH'].apply(\n",
    "    lambda x: pd.Series(extract_year_month(x))\n",
    ")\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "print(\"\\n训练集前5行:\")\n",
    "print(df_train[[\"MONTH\", \"YEAR\", \"MONTH_INT\"]].head())\n",
    "print(\"\\n测试集前5行:\")\n",
    "print(df_test[[\"MONTH\", \"YEAR\", \"MONTH_INT\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd1b99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 17)\n",
      "测试集形状: (50000, 22)\n",
      "\n",
      "FLAT_TYPE处理结果统计:\n",
      "训练集:\n",
      "FLAT_TYPE         IS_EXECUTIVE  NUM_ROOMS\n",
      "4 room            0             4            48459\n",
      "5 room            0             5            27916\n",
      "3 room            0             3            26702\n",
      "4-room            0             4            20414\n",
      "5-room            0             5            12167\n",
      "3-room            0             3            11904\n",
      "executive         1             0            11801\n",
      "2 room            0             2             2503\n",
      "2-room            0             2              700\n",
      "multi generation  0             0               68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集:\n",
      "FLAT_TYPE         IS_EXECUTIVE  NUM_ROOMS\n",
      "4 room            0             4            14939\n",
      "5 room            0             5             8483\n",
      "3 room            0             3             8307\n",
      "4-room            0             4             6187\n",
      "5-room            0             5             3749\n",
      "3-room            0             3             3685\n",
      "executive         1             0             3611\n",
      "2 room            0             2              783\n",
      "2-room            0             2              219\n",
      "multi generation  0             0               16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "训练集前10行:\n",
      "  FLAT_TYPE  IS_EXECUTIVE  NUM_ROOMS\n",
      "0    4 room             0          4\n",
      "1    4 room             0          4\n",
      "2    4 room             0          4\n",
      "3    4 room             0          4\n",
      "4    5 room             0          5\n",
      "5    4 room             0          4\n",
      "6    3 room             0          3\n",
      "7    3 room             0          3\n",
      "8    4 room             0          4\n",
      "9    3 room             0          3\n",
      "\n",
      "测试集前10行:\n",
      "  FLAT_TYPE  IS_EXECUTIVE  NUM_ROOMS\n",
      "0    3 room             0          3\n",
      "1    4 room             0          4\n",
      "2    3 room             0          3\n",
      "3    4 room             0          4\n",
      "4    4 room             0          4\n",
      "5    5-room             0          5\n",
      "6    4 room             0          4\n",
      "7    5 room             0          5\n",
      "8    4 room             0          4\n",
      "9    4-room             0          4\n"
     ]
    }
   ],
   "source": [
    "# 处理FLAT_TYPE列：拆分为IS_EXECUTIVE和NUM_ROOMS两列\n",
    "\n",
    "import re\n",
    "\n",
    "def process_flat_type(flat_type_str):\n",
    "    \"\"\"\n",
    "    处理FLAT_TYPE列：\n",
    "    - IS_EXECUTIVE: 如果是executive则为1，否则为0\n",
    "    - NUM_ROOMS: 如果是executive则为0，否则从\"N-room\"或\"N room\"格式中提取N\n",
    "    \"\"\"\n",
    "    flat_type_str = str(flat_type_str).strip().lower()\n",
    "    \n",
    "    # 检查是否为executive\n",
    "    if flat_type_str == 'executive':\n",
    "        return 1, 0\n",
    "    \n",
    "    # 尝试从\"N-room\"或\"N room\"格式中提取房间数\n",
    "    # 匹配模式：数字后跟\"-room\"或\" room\"\n",
    "    match = re.search(r'(\\d+)[\\s-]?room', flat_type_str)\n",
    "    if match:\n",
    "        num_rooms = int(match.group(1))\n",
    "        return 0, num_rooms\n",
    "    \n",
    "    # 如果无法匹配（如\"multi generation\"），返回0, 0\n",
    "    return 0, 0\n",
    "\n",
    "# 应用函数处理FLAT_TYPE列\n",
    "df_train[['IS_EXECUTIVE', 'NUM_ROOMS']] = df_train['FLAT_TYPE'].apply(\n",
    "    lambda x: pd.Series(process_flat_type(x))\n",
    ")\n",
    "df_test[['IS_EXECUTIVE', 'NUM_ROOMS']] = df_test['FLAT_TYPE'].apply(\n",
    "    lambda x: pd.Series(process_flat_type(x))\n",
    ")\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "print(\"\\nFLAT_TYPE处理结果统计:\")\n",
    "print(\"训练集:\")\n",
    "print(df_train[['FLAT_TYPE', 'IS_EXECUTIVE', 'NUM_ROOMS']].value_counts().head(10))\n",
    "print(\"\\n测试集:\")\n",
    "print(df_test[['FLAT_TYPE', 'IS_EXECUTIVE', 'NUM_ROOMS']].value_counts().head(10))\n",
    "print(\"\\n训练集前10行:\")\n",
    "print(df_train[['FLAT_TYPE', 'IS_EXECUTIVE', 'NUM_ROOMS']].head(10))\n",
    "print(\"\\n测试集前10行:\")\n",
    "print(df_test[['FLAT_TYPE', 'IS_EXECUTIVE', 'NUM_ROOMS']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3990d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 18)\n",
      "测试集形状: (50000, 23)\n",
      "\n",
      "FLOOR_RANGE编码结果统计:\n",
      "训练集前10行:\n",
      "  FLOOR_RANGE  FLOOR_RANGE_ORD\n",
      "0    07 to 09                2\n",
      "1    07 to 09                2\n",
      "2    19 to 21                6\n",
      "3    16 to 18                5\n",
      "4    10 to 12                3\n",
      "5    07 to 09                2\n",
      "6    13 to 15                4\n",
      "7    07 to 09                2\n",
      "8    04 to 06                1\n",
      "9    01 to 03                0\n",
      "\n",
      "测试集前10行:\n",
      "  FLOOR_RANGE  FLOOR_RANGE_ORD\n",
      "0    04 to 06                1\n",
      "1    01 to 03                0\n",
      "2    10 to 12                3\n",
      "3    10 to 12                3\n",
      "4    10 to 12                3\n",
      "5    04 to 06                1\n",
      "6    10 to 12                3\n",
      "7    01 to 03                0\n",
      "8    13 to 15                4\n",
      "9    04 to 06                1\n",
      "\n",
      "编码值分布（训练集）:\n",
      "FLOOR_RANGE_ORD\n",
      "0    28682\n",
      "1    37358\n",
      "2    34103\n",
      "3    30239\n",
      "4    15812\n",
      "5     7342\n",
      "6     3147\n",
      "7     2212\n",
      "8     1392\n",
      "9      887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 处理FLOOR_RANGE列：将楼层范围转换为有序编码\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "\n",
    "# 正则表达式匹配 \"数字 to 数字\" 格式\n",
    "_rng = re.compile(r'(\\d+)\\s*to\\s*(\\d+)', flags=re.IGNORECASE)\n",
    "\n",
    "def parse_floor_range(x):\n",
    "    \"\"\"解析楼层范围字符串，返回 (lo, hi) 元组\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    m = _rng.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    lo, hi = int(m.group(1)), int(m.group(2))\n",
    "    if lo > hi:\n",
    "        lo, hi = hi, lo\n",
    "    return lo, hi\n",
    "\n",
    "def build_floor_order_mapping(df_train, col=\"FLOOR_RANGE\"):\n",
    "    \"\"\"\n",
    "    基于训练集构建楼层范围的排序映射\n",
    "    返回: mapping字典, pairs列表, lows列表, median_code\n",
    "    \"\"\"\n",
    "    pairs = (\n",
    "        df_train[col]\n",
    "        .dropna()\n",
    "        .map(parse_floor_range)\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "    pairs = sorted(pairs, key=lambda t: (t[0], t[1]))\n",
    "    mapping = {f\"{lo:02d} to {hi:02d}\": i for i, (lo, hi) in enumerate(pairs)}\n",
    "    for i, (lo, hi) in enumerate(pairs):\n",
    "        mapping[f\"{lo} to {hi}\"] = i\n",
    "\n",
    "    lows = [lo for (lo, _) in pairs]\n",
    "    median_code = int(np.median(list(mapping.values()))) if mapping else 0\n",
    "    return mapping, pairs, lows, median_code\n",
    "\n",
    "def encode_floor_range_series(sr, mapping, pairs, lows, median_code):\n",
    "    \"\"\"\n",
    "    将楼层范围序列编码为有序整数\n",
    "    对于无法直接映射的值，使用fallback策略找到最接近的楼层范围\n",
    "    \"\"\"\n",
    "    enc = sr.map(mapping)\n",
    "    mask_na = enc.isna()\n",
    "    if mask_na.any():\n",
    "        parsed = sr[mask_na].map(parse_floor_range)\n",
    "\n",
    "        def fallback_code(p):\n",
    "            if p is None:\n",
    "                return median_code\n",
    "            lo, hi = p\n",
    "            if not lows:\n",
    "                return median_code\n",
    "            pos = bisect_left(lows, lo)\n",
    "            pos = max(0, min(pos, len(pairs)-1))\n",
    "            return pos\n",
    "\n",
    "        enc.loc[mask_na] = parsed.map(fallback_code)\n",
    "\n",
    "    return enc.astype(int)\n",
    "\n",
    "# 基于训练集构建映射\n",
    "mapping, pairs, lows, median_code = build_floor_order_mapping(df_train, col=\"FLOOR_RANGE\")\n",
    "\n",
    "# 应用编码到训练集和测试集\n",
    "df_train[\"FLOOR_RANGE_ORD\"] = encode_floor_range_series(\n",
    "    df_train[\"FLOOR_RANGE\"], mapping, pairs, lows, median_code\n",
    ")\n",
    "\n",
    "df_test[\"FLOOR_RANGE_ORD\"] = encode_floor_range_series(\n",
    "    df_test[\"FLOOR_RANGE\"], mapping, pairs, lows, median_code\n",
    ")\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "print(\"\\nFLOOR_RANGE编码结果统计:\")\n",
    "print(\"训练集前10行:\")\n",
    "print(df_train[[\"FLOOR_RANGE\", \"FLOOR_RANGE_ORD\"]].head(10))\n",
    "print(\"\\n测试集前10行:\")\n",
    "print(df_test[[\"FLOOR_RANGE\", \"FLOOR_RANGE_ORD\"]].head(10))\n",
    "print(\"\\n编码值分布（训练集）:\")\n",
    "print(df_train[\"FLOOR_RANGE_ORD\"].value_counts().sort_index().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3cd45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 19)\n",
      "测试集形状: (50000, 24)\n",
      "\n",
      "FLAT_MODEL唯一值数量: 21\n",
      "\n",
      "FLAT_MODEL唯一值列表:\n",
      " 1. 2 room\n",
      " 2. 3gen\n",
      " 3. adjoined flat\n",
      " 4. apartment\n",
      " 5. dbss\n",
      " 6. improved\n",
      " 7. improved maisonette\n",
      " 8. maisonette\n",
      " 9. model a\n",
      "10. model a maisonette\n",
      "11. model a2\n",
      "12. multi generation\n",
      "13. new generation\n",
      "14. premium apartment\n",
      "15. premium apartment loft\n",
      "16. premium maisonette\n",
      "17. simplified\n",
      "18. standard\n",
      "19. terrace\n",
      "20. type s1\n",
      "21. type s2\n",
      "\n",
      "训练集Target Encoding统计:\n",
      "          FLAT_MODEL  FLAT_MODEL_TARGET_ENC\n",
      "0  premium apartment          558599.442394\n",
      "1            model a          510078.105938\n",
      "2            model a          510293.613164\n",
      "3            model a          510754.108089\n",
      "4           improved          528016.555202\n",
      "5     new generation          380425.778851\n",
      "6            model a          510754.108089\n",
      "7     new generation          379755.152358\n",
      "8  premium apartment          558544.771495\n",
      "9            model a          510078.105938\n",
      "\n",
      "各FLAT_MODEL的Target Encoding值（按编码值排序）:\n",
      "FLAT_MODEL\n",
      "2 room                    3.492567e+05\n",
      "new generation            3.798808e+05\n",
      "simplified                4.008015e+05\n",
      "model a2                  4.143678e+05\n",
      "standard                  4.410008e+05\n",
      "model a                   5.105395e+05\n",
      "improved                  5.281605e+05\n",
      "premium apartment         5.591598e+05\n",
      "apartment                 7.089013e+05\n",
      "3gen                      7.286333e+05\n",
      "improved maisonette       7.612855e+05\n",
      "adjoined flat             7.668440e+05\n",
      "maisonette                7.697717e+05\n",
      "model a maisonette        7.871110e+05\n",
      "dbss                      7.883157e+05\n",
      "premium maisonette        8.082195e+05\n",
      "multi generation          8.615931e+05\n",
      "terrace                   8.749267e+05\n",
      "premium apartment loft    9.958932e+05\n",
      "type s1                   1.052140e+06\n",
      "type s2                   1.154374e+06\n",
      "Name: FLAT_MODEL_TARGET_ENC, dtype: float64\n",
      "\n",
      "测试集Target Encoding统计:\n",
      "   FLAT_MODEL  FLAT_MODEL_TARGET_ENC\n",
      "0     model a          510538.216210\n",
      "1     model a          510538.216210\n",
      "2    improved          528159.887103\n",
      "3  simplified          400799.181136\n",
      "4     model a          510538.216210\n",
      "5    improved          528159.887103\n",
      "6     model a          510538.216210\n",
      "7    improved          528159.887103\n",
      "8     model a          510538.216210\n",
      "9  simplified          400799.181136\n"
     ]
    }
   ],
   "source": [
    "# 处理FLAT_MODEL列：使用Target Encoding（K-Fold CV）\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def te_cv_train(df, col=\"FLAT_MODEL\", target_col=\"RESALE_PRICE\",\n",
    "                n_splits=5, out_col=None, random_state=0):\n",
    "    \"\"\"\n",
    "    K-Fold CV Target Encoding.\n",
    "    使用交叉验证避免数据泄露。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if out_col is None:\n",
    "        out_col = f\"{col}_TARGET_ENC\"\n",
    "    df[out_col] = pd.NA\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for tr_idx, va_idx in kf.split(df):\n",
    "        tr = df.iloc[tr_idx]\n",
    "        va = df.iloc[va_idx]\n",
    "\n",
    "        # 计算训练集中每个类别的目标变量均值\n",
    "        mean_map = tr.groupby(col)[target_col].mean().to_dict()\n",
    "        s = df.loc[va.index, col].map(mean_map)\n",
    "        # 对于未见过的类别，使用全局均值\n",
    "        fold_global = tr[target_col].mean()\n",
    "        s = s.fillna(fold_global)\n",
    "\n",
    "        df.loc[va.index, out_col] = s\n",
    "\n",
    "    df[out_col] = df[out_col].astype(float)\n",
    "    # 填充任何剩余的缺失值（使用全局均值）\n",
    "    df[out_col] = df[out_col].fillna(df[target_col].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 对训练集进行Target Encoding\n",
    "df_train = te_cv_train(df_train, col=\"FLAT_MODEL\", target_col=\"RESALE_PRICE\", \n",
    "                       n_splits=5, out_col=\"FLAT_MODEL_TARGET_ENC\")\n",
    "\n",
    "# 对测试集：使用训练集的统计信息进行编码\n",
    "# 计算训练集中每个FLAT_MODEL的RESALE_PRICE均值\n",
    "mean_map = df_train.groupby('FLAT_MODEL')['RESALE_PRICE'].mean().to_dict()\n",
    "global_mean = df_train['RESALE_PRICE'].mean()\n",
    "\n",
    "# 对测试集应用编码\n",
    "df_test['FLAT_MODEL_TARGET_ENC'] = df_test['FLAT_MODEL'].map(mean_map)\n",
    "# 对于测试集中未见过的类别，使用全局均值\n",
    "df_test['FLAT_MODEL_TARGET_ENC'] = df_test['FLAT_MODEL_TARGET_ENC'].fillna(global_mean)\n",
    "df_test['FLAT_MODEL_TARGET_ENC'] = df_test['FLAT_MODEL_TARGET_ENC'].astype(float)\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "print(f\"\\nFLAT_MODEL唯一值数量: {len(df_train['FLAT_MODEL'].unique())}\")\n",
    "print(\"\\nFLAT_MODEL唯一值列表:\")\n",
    "for i, model in enumerate(sorted(df_train['FLAT_MODEL'].unique()), 1):\n",
    "    print(f\"{i:2d}. {model}\")\n",
    "print(\"\\n训练集Target Encoding统计:\")\n",
    "print(df_train[[\"FLAT_MODEL\", \"FLAT_MODEL_TARGET_ENC\"]].head(10))\n",
    "print(\"\\n各FLAT_MODEL的Target Encoding值（按编码值排序）:\")\n",
    "encoding_stats = df_train.groupby('FLAT_MODEL')['FLAT_MODEL_TARGET_ENC'].mean().sort_values()\n",
    "print(encoding_stats)\n",
    "print(\"\\n测试集Target Encoding统计:\")\n",
    "print(df_test[[\"FLAT_MODEL\", \"FLAT_MODEL_TARGET_ENC\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e30388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 房龄特征统计 ===\n",
      "训练集房龄范围: 1 - 59 年\n",
      "测试集房龄范围: 1 - 58 年\n",
      "\n",
      "异常值检查:\n",
      "训练集中房龄为负数的样本数: 0\n",
      "测试集中房龄为负数的样本数: 0\n",
      "训练集中房龄超过99年的样本数: 0\n",
      "测试集中房龄超过99年的样本数: 0\n",
      "\n",
      "训练集房龄统计:\n",
      "count    162691.000000\n",
      "mean         24.691956\n",
      "std          14.137947\n",
      "min           1.000000\n",
      "25%          10.000000\n",
      "50%          25.000000\n",
      "75%          36.000000\n",
      "max          59.000000\n",
      "Name: FLAT_AGE, dtype: float64\n",
      "\n",
      "测试集房龄统计:\n",
      "count    50000.000000\n",
      "mean        24.650980\n",
      "std         14.159893\n",
      "min          1.000000\n",
      "25%         10.000000\n",
      "50%         25.000000\n",
      "75%         36.000000\n",
      "max         58.000000\n",
      "Name: FLAT_AGE, dtype: float64\n",
      "\n",
      "训练集房龄分布（前10个最常见的值）:\n",
      "FLAT_AGE\n",
      "1         1\n",
      "2        41\n",
      "3       811\n",
      "4      9871\n",
      "5     10545\n",
      "6      6729\n",
      "7      4684\n",
      "8      3157\n",
      "9      2790\n",
      "10     2210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 20)\n",
      "测试集形状: (50000, 25)\n",
      "\n",
      "示例数据（前10行）:\n",
      "   YEAR  LEASE_COMMENCE_DATA  FLAT_AGE  RESALE_PRICE\n",
      "0  2020                 2000        20      420000.0\n",
      "1  2021                 1992        29      585000.0\n",
      "2  2021                 1998        23      450000.0\n",
      "3  2021                 2017         4      465000.0\n",
      "4  2023                 2018         5      710000.0\n",
      "5  2024                 1980        44      590000.0\n",
      "6  2020                 2015         5      328000.0\n",
      "7  2025                 1979        46      488000.0\n",
      "8  2025                 2009        16      628888.0\n",
      "9  2025                 2018         7      480000.0\n"
     ]
    }
   ],
   "source": [
    "# 基于LEASE_COMMENCE_DATA创建房龄特征\n",
    "# 房龄 = 售卖年份 - 地契开始年份\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取训练集和测试集\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 计算房龄（从地契开始到售卖时经过的年数）\n",
    "df_train[\"FLAT_AGE\"] = df_train[\"YEAR\"] - df_train[\"LEASE_COMMENCE_DATA\"]\n",
    "df_test[\"FLAT_AGE\"] = df_test[\"YEAR\"] - df_test[\"LEASE_COMMENCE_DATA\"]\n",
    "\n",
    "# 检查是否有异常值（房龄为负数或过大）\n",
    "print(\"=== 房龄特征统计 ===\")\n",
    "print(f\"训练集房龄范围: {df_train['FLAT_AGE'].min()} - {df_train['FLAT_AGE'].max()} 年\")\n",
    "print(f\"测试集房龄范围: {df_test['FLAT_AGE'].min()} - {df_test['FLAT_AGE'].max()} 年\")\n",
    "\n",
    "# 检查异常值\n",
    "train_negative = (df_train[\"FLAT_AGE\"] < 0).sum()\n",
    "test_negative = (df_test[\"FLAT_AGE\"] < 0).sum()\n",
    "train_too_old = (df_train[\"FLAT_AGE\"] > 99).sum()  # 超过99年地契期限\n",
    "test_too_old = (df_test[\"FLAT_AGE\"] > 99).sum()\n",
    "\n",
    "print(f\"\\n异常值检查:\")\n",
    "print(f\"训练集中房龄为负数的样本数: {train_negative}\")\n",
    "print(f\"测试集中房龄为负数的样本数: {test_negative}\")\n",
    "print(f\"训练集中房龄超过99年的样本数: {train_too_old}\")\n",
    "print(f\"测试集中房龄超过99年的样本数: {test_too_old}\")\n",
    "\n",
    "# 显示房龄分布统计\n",
    "print(f\"\\n训练集房龄统计:\")\n",
    "print(df_train[\"FLAT_AGE\"].describe())\n",
    "print(f\"\\n测试集房龄统计:\")\n",
    "print(df_test[\"FLAT_AGE\"].describe())\n",
    "\n",
    "# 显示房龄分布（前10个最常见的值）\n",
    "print(f\"\\n训练集房龄分布（前10个最常见的值）:\")\n",
    "print(df_train[\"FLAT_AGE\"].value_counts().sort_index().head(10))\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"\\n文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "\n",
    "# 显示示例数据\n",
    "print(\"\\n示例数据（前10行）:\")\n",
    "print(df_train[[\"YEAR\", \"LEASE_COMMENCE_DATA\", \"FLAT_AGE\", \"RESALE_PRICE\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dba9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中存在的要删除的列: ['MONTH', 'FLAT_TYPE', 'FLOOR_RANGE', 'FLAT_MODEL', 'ECO_CATEGORY']\n",
      "测试集中存在的要删除的列: ['MONTH', 'FLAT_TYPE', 'FLOOR_RANGE', 'FLAT_MODEL', 'ECO_CATEGORY']\n",
      "\n",
      "从训练集中删除了列: ['MONTH', 'FLAT_TYPE', 'FLOOR_RANGE', 'FLAT_MODEL', 'ECO_CATEGORY']\n",
      "从测试集中删除了列: ['MONTH', 'FLAT_TYPE', 'FLOOR_RANGE', 'FLAT_MODEL', 'ECO_CATEGORY']\n",
      "\n",
      "文件已更新：train.csv 和 test.csv\n",
      "训练集新形状: (162691, 15)\n",
      "测试集新形状: (50000, 20)\n",
      "\n",
      "训练集剩余列名:\n",
      "['TOWN', 'BLOCK', 'STREET', 'FLOOR_AREA_SQM', 'LEASE_COMMENCE_DATA', 'RESALE_PRICE', 'LATITUDE', 'LONGITUDE', 'YEAR', 'MONTH_INT', 'IS_EXECUTIVE', 'NUM_ROOMS', 'FLOOR_RANGE_ORD', 'FLAT_MODEL_TARGET_ENC', 'FLAT_AGE']\n",
      "\n",
      "测试集剩余列名:\n",
      "['TOWN', 'BLOCK', 'STREET', 'FLOOR_AREA_SQM', 'LEASE_COMMENCE_DATA', 'DIST_MRT_MIN', 'DIST_PRIMARY_SCHOOL_MIN', 'DIST_SECONDARY_SCHOOL_MIN', 'DIST_MALL_MIN', 'DIST_HAWKER_MIN', 'address', 'LATITUDE', 'LONGITUDE', 'YEAR', 'MONTH_INT', 'IS_EXECUTIVE', 'NUM_ROOMS', 'FLOOR_RANGE_ORD', 'FLAT_MODEL_TARGET_ENC', 'FLAT_AGE']\n"
     ]
    }
   ],
   "source": [
    "# 删除不需要的列：\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取训练集和测试集\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 要删除的列\n",
    "cols_to_drop = ['MONTH', 'FLAT_TYPE', 'FLOOR_RANGE', 'FLAT_MODEL', 'ECO_CATEGORY']\n",
    "\n",
    "# 检查哪些列存在\n",
    "train_cols_exist = [col for col in cols_to_drop if col in df_train.columns]\n",
    "test_cols_exist = [col for col in cols_to_drop if col in df_test.columns]\n",
    "\n",
    "print(f\"训练集中存在的要删除的列: {train_cols_exist}\")\n",
    "print(f\"测试集中存在的要删除的列: {test_cols_exist}\")\n",
    "\n",
    "# 删除列\n",
    "if train_cols_exist:\n",
    "    df_train = df_train.drop(columns=train_cols_exist)\n",
    "    print(f\"\\n从训练集中删除了列: {train_cols_exist}\")\n",
    "\n",
    "if test_cols_exist:\n",
    "    df_test = df_test.drop(columns=test_cols_exist)\n",
    "    print(f\"从测试集中删除了列: {test_cols_exist}\")\n",
    "\n",
    "# 保存修改后的文件（覆盖原文件）\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"\\n文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集新形状: {df_train.shape}\")\n",
    "print(f\"测试集新形状: {df_test.shape}\")\n",
    "print(f\"\\n训练集剩余列名:\")\n",
    "print(df_train.columns.tolist())\n",
    "print(f\"\\n测试集剩余列名:\")\n",
    "print(df_test.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4406fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "============================================================\n",
      "计算每个数据点到各类型设施最近的5个距离...\n",
      "============================================================\n",
      "\n",
      "处理 hawker (sg-gov-hawkers.csv):\n",
      "  原始数据点数: 107\n",
      "  去重后数据点数: 106 (去除了 1 个重复位置)\n",
      "  计算训练集最近的5个距离...\n",
      "  计算测试集最近的5个距离...\n",
      "  已创建 5 个特征列\n",
      "\n",
      "处理 station (sg-mrt-stations.csv):\n",
      "  原始数据点数: 243\n",
      "  去重后数据点数: 192 (去除了 51 个重复位置)\n",
      "  计算训练集最近的5个距离...\n",
      "  计算测试集最近的5个距离...\n",
      "  已创建 5 个特征列\n",
      "\n",
      "处理 primaryschool (sg-primary-schools.csv):\n",
      "  原始数据点数: 182\n",
      "  去重后数据点数: 180 (去除了 2 个重复位置)\n",
      "  计算训练集最近的5个距离...\n",
      "  计算测试集最近的5个距离...\n",
      "  已创建 5 个特征列\n",
      "\n",
      "处理 secondaryschool (sg-secondary-schools.csv):\n",
      "  原始数据点数: 153\n",
      "  去重后数据点数: 152 (去除了 1 个重复位置)\n",
      "  计算训练集最近的5个距离...\n",
      "  计算测试集最近的5个距离...\n",
      "  已创建 5 个特征列\n",
      "\n",
      "处理 shoppingmall (sg-shopping-malls.csv):\n",
      "  原始数据点数: 89\n",
      "  去重后数据点数: 88 (去除了 1 个重复位置)\n",
      "  计算训练集最近的5个距离...\n",
      "  计算测试集最近的5个距离...\n",
      "  已创建 5 个特征列\n",
      "\n",
      "============================================================\n",
      "统一标准化所有最近距离特征（仅使用训练集计算标准化参数）...\n",
      "训练集距离范围: [0.0032, 10.4481] 公里\n",
      "训练集距离值数量: 4067275\n",
      "注意: 使用训练集的min/max来标准化训练集和测试集，避免数据泄露\n",
      "\n",
      "标准化所有最近距离特征...\n",
      "\n",
      "保存文件...\n",
      "\n",
      "============================================================\n",
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 40)\n",
      "测试集形状: (50000, 45)\n",
      "\n",
      "创建的最近距离特征数: 25 个\n",
      "\n",
      "特征列名示例（前10个）:\n",
      "   1. DIST_NEAREST_5_HAWKER_1: min=0.3320, max=0.9969, mean=0.8172\n",
      "   2. DIST_NEAREST_5_HAWKER_2: min=0.2930, max=0.9910, mean=0.7474\n",
      "   3. DIST_NEAREST_5_HAWKER_3: min=0.2377, max=0.9720, mean=0.6996\n",
      "   4. DIST_NEAREST_5_HAWKER_4: min=0.0086, max=0.9647, mean=0.6419\n",
      "   5. DIST_NEAREST_5_HAWKER_5: min=0.0000, max=0.9641, mean=0.6153\n",
      "   6. DIST_NEAREST_5_STATION_1: min=0.7371, max=0.9983, mean=0.9381\n",
      "   7. DIST_NEAREST_5_STATION_2: min=0.7282, max=0.9780, mean=0.8946\n",
      "   8. DIST_NEAREST_5_STATION_3: min=0.6791, max=0.9665, mean=0.8550\n",
      "   9. DIST_NEAREST_5_STATION_4: min=0.5753, max=0.9567, mean=0.8201\n",
      "  10. DIST_NEAREST_5_STATION_5: min=0.5137, max=0.9561, mean=0.7918\n",
      "\n",
      "所有列数: 40\n"
     ]
    }
   ],
   "source": [
    "# 计算每个数据点到各类型设施最近的5个距离的标准化值作为特征\n",
    "# 对于每种设施类型（hawker, station, primaryschool, secondaryschool, shoppingmall），\n",
    "# 找到每个数据点最近的5个设施，计算距离并标准化\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "def haversine_torch(lat1, lon1, lat2, lon2, device=device):\n",
    "    \"\"\"\n",
    "    使用PyTorch计算Haversine距离（单位：公里）\n",
    "    \"\"\"\n",
    "    lat1 = torch.deg2rad(torch.tensor(lat1, dtype=torch.float32, device=device))\n",
    "    lon1 = torch.deg2rad(torch.tensor(lon1, dtype=torch.float32, device=device))\n",
    "    lat2 = torch.deg2rad(torch.tensor(lat2, dtype=torch.float32, device=device))\n",
    "    lon2 = torch.deg2rad(torch.tensor(lon2, dtype=torch.float32, device=device))\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon/2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))\n",
    "    \n",
    "    R = 6371.0  # 地球半径（公里）\n",
    "    return R * c\n",
    "\n",
    "def calculate_nearest_k_distances(df_main, df_aux, k=5, lat_col='LATITUDE', lon_col='LONGITUDE', device=device):\n",
    "    \"\"\"\n",
    "    计算主数据集中每个点到辅助数据集中最近的k个点的距离（完全向量化版本）\n",
    "    返回: numpy数组，形状为 (len(df_main), k)，每行包含最近的k个距离（按距离从小到大排序）\n",
    "    \"\"\"\n",
    "    # 提取主数据集的经纬度并转换为tensor\n",
    "    main_lat = torch.tensor(df_main[lat_col].values, dtype=torch.float32, device=device)\n",
    "    main_lon = torch.tensor(df_main[lon_col].values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 提取辅助数据集的经纬度并转换为tensor\n",
    "    aux_lat = torch.tensor(df_aux[lat_col].values, dtype=torch.float32, device=device)\n",
    "    aux_lon = torch.tensor(df_aux[lon_col].values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 创建掩码，标记有效的经纬度\n",
    "    main_valid = ~(torch.isnan(main_lat) | torch.isnan(main_lon))\n",
    "    aux_valid = ~(torch.isnan(aux_lat) | torch.isnan(aux_lon))\n",
    "    \n",
    "    # 过滤掉无效的辅助数据点\n",
    "    aux_lat_valid = aux_lat[aux_valid]\n",
    "    aux_lon_valid = aux_lon[aux_valid]\n",
    "    \n",
    "    if len(aux_lat_valid) == 0:\n",
    "        return np.full((len(df_main), k), np.nan)\n",
    "    \n",
    "    # 转换为弧度\n",
    "    main_lat_rad = torch.deg2rad(main_lat)\n",
    "    main_lon_rad = torch.deg2rad(main_lon)\n",
    "    aux_lat_rad = torch.deg2rad(aux_lat_valid)\n",
    "    aux_lon_rad = torch.deg2rad(aux_lon_valid)\n",
    "    \n",
    "    # 使用广播计算所有距离矩阵\n",
    "    # main_lat_rad: (N,), aux_lat_rad: (M,)\n",
    "    # 扩展维度以进行广播: (N, 1) 和 (1, M) -> (N, M)\n",
    "    main_lat_expanded = main_lat_rad.unsqueeze(1)  # (N, 1)\n",
    "    main_lon_expanded = main_lon_rad.unsqueeze(1)  # (N, 1)\n",
    "    aux_lat_expanded = aux_lat_rad.unsqueeze(0)    # (1, M)\n",
    "    aux_lon_expanded = aux_lon_rad.unsqueeze(0)    # (1, M)\n",
    "    \n",
    "    # 计算Haversine距离（向量化）\n",
    "    dlat = aux_lat_expanded - main_lat_expanded  # (N, M)\n",
    "    dlon = aux_lon_expanded - main_lon_expanded  # (N, M)\n",
    "    \n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(main_lat_expanded) * torch.cos(aux_lat_expanded) * torch.sin(dlon/2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))\n",
    "    R = 6371.0  # 地球半径（公里）\n",
    "    distances_matrix = R * c  # (N, M)\n",
    "    \n",
    "    # 将无效主数据点的距离设为inf，这样它们不会被选为最近的点\n",
    "    distances_matrix[~main_valid, :] = float('inf')\n",
    "    \n",
    "    # 转换为numpy数组以便使用numpy的排序函数\n",
    "    if device == 'cuda':\n",
    "        distances_matrix = distances_matrix.cpu().numpy()\n",
    "    else:\n",
    "        distances_matrix = distances_matrix.numpy()\n",
    "    \n",
    "    # 找到每个主数据点最近的k个距离\n",
    "    k_actual = min(k, distances_matrix.shape[1])\n",
    "    if k_actual == 0:\n",
    "        return np.full((len(df_main), k), np.nan)\n",
    "    \n",
    "    # 使用argpartition找到最小的k个元素（比完全排序更快）\n",
    "    nearest_indices = np.argpartition(distances_matrix, k_actual - 1, axis=1)[:, :k_actual]\n",
    "    \n",
    "    # 提取对应的距离值并排序\n",
    "    nearest_distances = np.full((len(df_main), k), np.nan)\n",
    "    for i in range(len(df_main)):\n",
    "        if not main_valid[i]:\n",
    "            continue\n",
    "        nearest_k = distances_matrix[i, nearest_indices[i]]\n",
    "        nearest_k = np.sort(nearest_k)\n",
    "        # 如果找到的距离少于k个，用最后一个距离填充\n",
    "        if len(nearest_k) < k:\n",
    "            nearest_k = np.pad(nearest_k, (0, k - len(nearest_k)), mode='edge')\n",
    "        nearest_distances[i, :] = nearest_k[:k]\n",
    "    \n",
    "    return nearest_distances\n",
    "\n",
    "# 读取训练集和测试集\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 定义设施类型和对应的文件名映射\n",
    "facility_mapping = {\n",
    "    'hawker': 'sg-gov-hawkers.csv',\n",
    "    'station': 'sg-mrt-stations.csv',\n",
    "    'primaryschool': 'sg-primary-schools.csv',\n",
    "    'secondaryschool': 'sg-secondary-schools.csv',\n",
    "    'shoppingmall': 'sg-shopping-malls.csv'\n",
    "}\n",
    "\n",
    "auxiliary_dir = \"auxiliary-data\"\n",
    "k_nearest = 5  # 最近的5个距离\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"计算每个数据点到各类型设施最近的{k_nearest}个距离...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 收集训练集距离用于计算标准化参数（避免数据泄露）\n",
    "train_distances = []\n",
    "\n",
    "# 为每种设施类型计算最近的k个距离\n",
    "for facility_type, filename in facility_mapping.items():\n",
    "    aux_file = os.path.join(auxiliary_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(aux_file):\n",
    "        print(f\"\\n警告: 文件 {filename} 不存在，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 读取辅助数据\n",
    "    df_aux = pd.read_csv(aux_file)\n",
    "    \n",
    "    # 检查是否有LATITUDE和LONGITUDE列\n",
    "    if 'LATITUDE' not in df_aux.columns or 'LONGITUDE' not in df_aux.columns:\n",
    "        print(f\"\\n警告: {filename} 缺少LATITUDE或LONGITUDE列，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 过滤掉缺失经纬度的行\n",
    "    df_aux = df_aux.dropna(subset=['LATITUDE', 'LONGITUDE']).reset_index(drop=True)\n",
    "    \n",
    "    if len(df_aux) == 0:\n",
    "        print(f\"\\n警告: {filename} 没有有效的经纬度数据，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 对重复的经纬度进行去重（同一个位置只保留一个）\n",
    "    # 这对于MRT站很重要，因为同一个站可能有多条线路\n",
    "    original_count = len(df_aux)\n",
    "    df_aux = df_aux.drop_duplicates(subset=['LATITUDE', 'LONGITUDE']).reset_index(drop=True)\n",
    "    unique_count = len(df_aux)\n",
    "    \n",
    "    print(f\"\\n处理 {facility_type} ({filename}):\")\n",
    "    print(f\"  原始数据点数: {original_count}\")\n",
    "    print(f\"  去重后数据点数: {unique_count} (去除了 {original_count - unique_count} 个重复位置)\")\n",
    "    \n",
    "    # 计算训练集最近的k个距离\n",
    "    print(\"  计算训练集最近的5个距离...\")\n",
    "    train_nearest = calculate_nearest_k_distances(df_train, df_aux, k=k_nearest, device=device)\n",
    "    \n",
    "    # 计算测试集最近的k个距离\n",
    "    print(\"  计算测试集最近的5个距离...\")\n",
    "    test_nearest = calculate_nearest_k_distances(df_test, df_aux, k=k_nearest, device=device)\n",
    "    \n",
    "    # 只收集训练集距离用于计算标准化参数（避免数据泄露）\n",
    "    train_valid = ~np.isnan(train_nearest)\n",
    "    train_distances.extend(train_nearest[train_valid].tolist())\n",
    "    \n",
    "    # 创建特征列名并添加到DataFrame\n",
    "    for i in range(k_nearest):\n",
    "        col_name = f\"DIST_NEAREST_{k_nearest}_{facility_type.upper()}_{i+1}\"\n",
    "        df_train[col_name] = train_nearest[:, i]\n",
    "        df_test[col_name] = test_nearest[:, i]\n",
    "    \n",
    "    print(f\"  已创建 {k_nearest} 个特征列\")\n",
    "\n",
    "# 统一标准化：只使用训练集距离计算min和max（避免数据泄露）\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"统一标准化所有最近距离特征（仅使用训练集计算标准化参数）...\")\n",
    "train_distances = np.array(train_distances)\n",
    "dmin = np.min(train_distances)\n",
    "dmax = np.max(train_distances)\n",
    "\n",
    "print(f\"训练集距离范围: [{dmin:.4f}, {dmax:.4f}] 公里\")\n",
    "print(f\"训练集距离值数量: {len(train_distances)}\")\n",
    "print(\"注意: 使用训练集的min/max来标准化训练集和测试集，避免数据泄露\")\n",
    "\n",
    "# 对每个最近距离特征进行标准化\n",
    "print(\"\\n标准化所有最近距离特征...\")\n",
    "for facility_type in facility_mapping.keys():\n",
    "    for i in range(k_nearest):\n",
    "        col_name = f\"DIST_NEAREST_{k_nearest}_{facility_type.upper()}_{i+1}\"\n",
    "        if col_name in df_train.columns:\n",
    "            # 标准化公式: 1 - (d - dmin) / (dmax - dmin)\n",
    "            if dmax > dmin:\n",
    "                df_train[col_name] = (1 - (df_train[col_name] - dmin) / (dmax - dmin)).fillna(0.0)\n",
    "                df_test[col_name] = (1 - (df_test[col_name] - dmin) / (dmax - dmin)).fillna(0.0)\n",
    "            else:\n",
    "                df_train[col_name] = df_train[col_name].fillna(1.0)\n",
    "                df_test[col_name] = df_test[col_name].fillna(1.0)\n",
    "\n",
    "# 保存修改后的文件\n",
    "print(\"\\n保存文件...\")\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "\n",
    "# 显示创建的特征统计\n",
    "print(f\"\\n创建的最近距离特征数: {len(facility_mapping) * k_nearest} 个\")\n",
    "print(\"\\n特征列名示例（前10个）:\")\n",
    "nearest_cols = [col for col in df_train.columns if col.startswith('DIST_NEAREST_')]\n",
    "for i, col in enumerate(nearest_cols[:10], 1):\n",
    "    if col in df_train.columns:\n",
    "        print(f\"  {i:2d}. {col}: min={df_train[col].min():.4f}, max={df_train[col].max():.4f}, mean={df_train[col].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n所有列数: {len(df_train.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd088f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "============================================================\n",
      "计算在指定距离范围内（[0.5, 1, 2, 3, 5, 10]km）的各类型设施数量...\n",
      "============================================================\n",
      "\n",
      "处理 hawker (sg-gov-hawkers.csv):\n",
      "  原始数据点数: 107\n",
      "  去重后数据点数: 106 (去除了 1 个重复位置)\n",
      "  计算训练集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  计算测试集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  已创建 6 个特征列\n",
      "  训练集统计（前3个阈值）:\n",
      "    0.5km: 平均=0.40, 最大=5\n",
      "    1km: 平均=1.22, 最大=8\n",
      "    2km: 平均=3.25, 最大=18\n",
      "\n",
      "处理 station (sg-mrt-stations.csv):\n",
      "  原始数据点数: 243\n",
      "  去重后数据点数: 192 (去除了 51 个重复位置)\n",
      "  计算训练集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  计算测试集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  已创建 6 个特征列\n",
      "  训练集统计（前3个阈值）:\n",
      "    0.5km: 平均=0.43, 最大=5\n",
      "    1km: 平均=1.46, 最大=9\n",
      "    2km: 平均=5.07, 最大=25\n",
      "\n",
      "处理 primaryschool (sg-primary-schools.csv):\n",
      "  原始数据点数: 182\n",
      "  去重后数据点数: 180 (去除了 2 个重复位置)\n",
      "  计算训练集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  计算测试集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  已创建 6 个特征列\n",
      "  训练集统计（前3个阈值）:\n",
      "    0.5km: 平均=1.02, 最大=3\n",
      "    1km: 平均=3.15, 最大=8\n",
      "    2km: 平均=8.68, 最大=19\n",
      "\n",
      "处理 secondaryschool (sg-secondary-schools.csv):\n",
      "  原始数据点数: 153\n",
      "  去重后数据点数: 152 (去除了 1 个重复位置)\n",
      "  计算训练集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  计算测试集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  已创建 6 个特征列\n",
      "  训练集统计（前3个阈值）:\n",
      "    0.5km: 平均=0.71, 最大=3\n",
      "    1km: 平均=2.29, 最大=7\n",
      "    2km: 平均=6.63, 最大=12\n",
      "\n",
      "处理 shoppingmall (sg-shopping-malls.csv):\n",
      "  原始数据点数: 89\n",
      "  去重后数据点数: 88 (去除了 1 个重复位置)\n",
      "  计算训练集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  计算测试集在 [0.5, 1, 2, 3, 5, 10]km 范围内的设施数量...\n",
      "  已创建 6 个特征列\n",
      "  训练集统计（前3个阈值）:\n",
      "    0.5km: 平均=0.29, 最大=6\n",
      "    1km: 平均=0.91, 最大=12\n",
      "    2km: 平均=2.51, 最大=27\n",
      "\n",
      "============================================================\n",
      "保存文件...\n",
      "\n",
      "============================================================\n",
      "文件已更新：train.csv 和 test.csv\n",
      "训练集形状: (162691, 70)\n",
      "测试集形状: (50000, 75)\n",
      "\n",
      "创建的距离范围内设施数量特征数: 30 个\n",
      "\n",
      "特征列名示例（前10个）:\n",
      "   1. COUNT_WITHIN_0.5KM_HAWKER: min=0, max=5, mean=0.40\n",
      "   2. COUNT_WITHIN_1KM_HAWKER: min=0, max=8, mean=1.22\n",
      "   3. COUNT_WITHIN_2KM_HAWKER: min=0, max=18, mean=3.25\n",
      "   4. COUNT_WITHIN_3KM_HAWKER: min=0, max=25, mean=5.82\n",
      "   5. COUNT_WITHIN_5KM_HAWKER: min=0, max=46, mean=13.00\n",
      "   6. COUNT_WITHIN_10KM_HAWKER: min=3, max=92, mean=43.38\n",
      "   7. COUNT_WITHIN_0.5KM_STATION: min=0, max=5, mean=0.43\n",
      "   8. COUNT_WITHIN_1KM_STATION: min=0, max=9, mean=1.46\n",
      "   9. COUNT_WITHIN_2KM_STATION: min=0, max=25, mean=5.07\n",
      "  10. COUNT_WITHIN_3KM_STATION: min=2, max=40, mean=10.20\n",
      "\n",
      "所有列数: 70\n"
     ]
    }
   ],
   "source": [
    "# 计算在指定距离范围内（1km, 3km, 5km, 10km, 20km）的各类型设施数量\n",
    "# 对于每种设施类型，统计每个数据点在指定距离阈值内的设施数量\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "def calculate_facility_counts_within_radius(df_main, df_aux, distance_thresholds, \n",
    "                                            lat_col='LATITUDE', lon_col='LONGITUDE', device=device):\n",
    "    \"\"\"\n",
    "    计算主数据集中每个点在指定距离阈值内的辅助数据点数量（完全向量化版本）\n",
    "    \n",
    "    参数:\n",
    "        df_main: 主数据集（训练集或测试集）\n",
    "        df_aux: 辅助数据集（设施数据）\n",
    "        distance_thresholds: 距离阈值列表（单位：公里），例如 [1, 3, 5, 10, 20]\n",
    "        lat_col: 纬度列名\n",
    "        lon_col: 经度列名\n",
    "        device: 计算设备（'cuda' 或 'cpu'）\n",
    "    \n",
    "    返回:\n",
    "        numpy数组，形状为 (len(df_main), len(distance_thresholds))，\n",
    "        每行包含在每个距离阈值内的设施数量\n",
    "    \"\"\"\n",
    "    # 提取主数据集的经纬度并转换为tensor\n",
    "    main_lat = torch.tensor(df_main[lat_col].values, dtype=torch.float32, device=device)\n",
    "    main_lon = torch.tensor(df_main[lon_col].values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 提取辅助数据集的经纬度并转换为tensor\n",
    "    aux_lat = torch.tensor(df_aux[lat_col].values, dtype=torch.float32, device=device)\n",
    "    aux_lon = torch.tensor(df_aux[lon_col].values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 创建掩码，标记有效的经纬度\n",
    "    main_valid = ~(torch.isnan(main_lat) | torch.isnan(main_lon))\n",
    "    aux_valid = ~(torch.isnan(aux_lat) | torch.isnan(aux_lon))\n",
    "    \n",
    "    # 过滤掉无效的辅助数据点\n",
    "    aux_lat_valid = aux_lat[aux_valid]\n",
    "    aux_lon_valid = aux_lon[aux_valid]\n",
    "    \n",
    "    if len(aux_lat_valid) == 0:\n",
    "        return np.zeros((len(df_main), len(distance_thresholds)))\n",
    "    \n",
    "    # 转换为弧度\n",
    "    main_lat_rad = torch.deg2rad(main_lat)\n",
    "    main_lon_rad = torch.deg2rad(main_lon)\n",
    "    aux_lat_rad = torch.deg2rad(aux_lat_valid)\n",
    "    aux_lon_rad = torch.deg2rad(aux_lon_valid)\n",
    "    \n",
    "    # 使用广播计算所有距离矩阵\n",
    "    # main_lat_rad: (N,), aux_lat_rad: (M,)\n",
    "    # 扩展维度以进行广播: (N, 1) 和 (1, M) -> (N, M)\n",
    "    main_lat_expanded = main_lat_rad.unsqueeze(1)  # (N, 1)\n",
    "    main_lon_expanded = main_lon_rad.unsqueeze(1)  # (N, 1)\n",
    "    aux_lat_expanded = aux_lat_rad.unsqueeze(0)    # (1, M)\n",
    "    aux_lon_expanded = aux_lon_rad.unsqueeze(0)    # (1, M)\n",
    "    \n",
    "    # 计算Haversine距离（向量化）\n",
    "    dlat = aux_lat_expanded - main_lat_expanded  # (N, M)\n",
    "    dlon = aux_lon_expanded - main_lon_expanded  # (N, M)\n",
    "    \n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(main_lat_expanded) * torch.cos(aux_lat_expanded) * torch.sin(dlon/2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))\n",
    "    R = 6371.0  # 地球半径（公里）\n",
    "    distances_matrix = R * c  # (N, M)\n",
    "    \n",
    "    # 将无效主数据点的距离设为inf，这样它们不会被计入\n",
    "    distances_matrix[~main_valid, :] = float('inf')\n",
    "    \n",
    "    # 转换为numpy数组以便使用numpy的计数函数\n",
    "    if device == 'cuda':\n",
    "        distances_matrix = distances_matrix.cpu().numpy()\n",
    "    else:\n",
    "        distances_matrix = distances_matrix.numpy()\n",
    "    \n",
    "    # 对于每个距离阈值，统计在范围内的设施数量\n",
    "    counts = np.zeros((len(df_main), len(distance_thresholds)))\n",
    "    for i, threshold in enumerate(distance_thresholds):\n",
    "        # 统计每个主数据点在阈值内的设施数量\n",
    "        within_threshold = distances_matrix <= threshold\n",
    "        counts[:, i] = within_threshold.sum(axis=1)\n",
    "    \n",
    "    # 对于无效的主数据点，将计数设为0\n",
    "    if device == 'cuda':\n",
    "        main_valid_np = main_valid.cpu().numpy()\n",
    "    else:\n",
    "        main_valid_np = main_valid.numpy()\n",
    "    counts[~main_valid_np, :] = 0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# 读取训练集和测试集\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 定义设施类型和对应的文件名映射\n",
    "facility_mapping = {\n",
    "    'hawker': 'sg-gov-hawkers.csv',\n",
    "    'station': 'sg-mrt-stations.csv',\n",
    "    'primaryschool': 'sg-primary-schools.csv',\n",
    "    'secondaryschool': 'sg-secondary-schools.csv',\n",
    "    'shoppingmall': 'sg-shopping-malls.csv'\n",
    "}\n",
    "\n",
    "# 定义距离阈值（单位：公里）\n",
    "distance_thresholds = [0.5, 1, 2, 3, 5, 10]\n",
    "\n",
    "auxiliary_dir = \"auxiliary-data\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"计算在指定距离范围内（{distance_thresholds}km）的各类型设施数量...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 为每种设施类型计算在指定距离范围内的数量\n",
    "for facility_type, filename in facility_mapping.items():\n",
    "    aux_file = os.path.join(auxiliary_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(aux_file):\n",
    "        print(f\"\\n警告: 文件 {filename} 不存在，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 读取辅助数据\n",
    "    df_aux = pd.read_csv(aux_file)\n",
    "    \n",
    "    # 检查是否有LATITUDE和LONGITUDE列\n",
    "    if 'LATITUDE' not in df_aux.columns or 'LONGITUDE' not in df_aux.columns:\n",
    "        print(f\"\\n警告: {filename} 缺少LATITUDE或LONGITUDE列，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 过滤掉缺失经纬度的行\n",
    "    df_aux = df_aux.dropna(subset=['LATITUDE', 'LONGITUDE']).reset_index(drop=True)\n",
    "    \n",
    "    if len(df_aux) == 0:\n",
    "        print(f\"\\n警告: {filename} 没有有效的经纬度数据，跳过 {facility_type}\")\n",
    "        continue\n",
    "    \n",
    "    # 对重复的经纬度进行去重（同一个位置只保留一个）\n",
    "    # 这对于MRT站很重要，因为同一个站可能有多条线路\n",
    "    original_count = len(df_aux)\n",
    "    df_aux = df_aux.drop_duplicates(subset=['LATITUDE', 'LONGITUDE']).reset_index(drop=True)\n",
    "    unique_count = len(df_aux)\n",
    "    \n",
    "    print(f\"\\n处理 {facility_type} ({filename}):\")\n",
    "    print(f\"  原始数据点数: {original_count}\")\n",
    "    print(f\"  去重后数据点数: {unique_count} (去除了 {original_count - unique_count} 个重复位置)\")\n",
    "    \n",
    "    # 计算训练集在指定距离范围内的设施数量\n",
    "    print(f\"  计算训练集在 {distance_thresholds}km 范围内的设施数量...\")\n",
    "    train_counts = calculate_facility_counts_within_radius(\n",
    "        df_train, df_aux, distance_thresholds, device=device\n",
    "    )\n",
    "    \n",
    "    # 计算测试集在指定距离范围内的设施数量\n",
    "    print(f\"  计算测试集在 {distance_thresholds}km 范围内的设施数量...\")\n",
    "    test_counts = calculate_facility_counts_within_radius(\n",
    "        df_test, df_aux, distance_thresholds, device=device\n",
    "    )\n",
    "    \n",
    "    # 创建特征列并添加到DataFrame\n",
    "    for i, threshold in enumerate(distance_thresholds):\n",
    "        col_name = f\"COUNT_WITHIN_{threshold}KM_{facility_type.upper()}\"\n",
    "        df_train[col_name] = train_counts[:, i]\n",
    "        df_test[col_name] = test_counts[:, i]\n",
    "    \n",
    "    print(f\"  已创建 {len(distance_thresholds)} 个特征列\")\n",
    "    \n",
    "    # 显示统计信息\n",
    "    print(f\"  训练集统计（前3个阈值）:\")\n",
    "    for i, threshold in enumerate(distance_thresholds[:3]):\n",
    "        col_name = f\"COUNT_WITHIN_{threshold}KM_{facility_type.upper()}\"\n",
    "        mean_count = df_train[col_name].mean()\n",
    "        max_count = df_train[col_name].max()\n",
    "        print(f\"    {threshold}km: 平均={mean_count:.2f}, 最大={max_count:.0f}\")\n",
    "\n",
    "# 保存修改后的文件\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"保存文件...\")\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"文件已更新：train.csv 和 test.csv\")\n",
    "print(f\"训练集形状: {df_train.shape}\")\n",
    "print(f\"测试集形状: {df_test.shape}\")\n",
    "\n",
    "# 显示创建的特征统计\n",
    "print(f\"\\n创建的距离范围内设施数量特征数: {len(facility_mapping) * len(distance_thresholds)} 个\")\n",
    "print(\"\\n特征列名示例（前10个）:\")\n",
    "count_cols = [col for col in df_train.columns if col.startswith('COUNT_WITHIN_')]\n",
    "for i, col in enumerate(count_cols[:10], 1):\n",
    "    if col in df_train.columns:\n",
    "        print(f\"  {i:2d}. {col}: min={df_train[col].min():.0f}, max={df_train[col].max():.0f}, mean={df_train[col].mean():.2f}\")\n",
    "\n",
    "print(f\"\\n所有列数: {len(df_train.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c0676c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM grid:   0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: TOWN: object, BLOCK: object, STREET: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 86\u001b[0m\n\u001b[1;32m     72\u001b[0m lgbm_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m700\u001b[39m],\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     81\u001b[0m }\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# rf_best_params, rf_cv = cv_search(\"RandomForest\", RandomForestRegressor, rf_grid, X, y)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m lgbm_best_params, lgbm_cv \u001b[38;5;241m=\u001b[39m \u001b[43mcv_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightGBM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLGBMRegressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgbm_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# rf_preds   = train_full_and_predict(RandomForestRegressor, rf_best_params, X, y, X_test)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m lgbm_preds \u001b[38;5;241m=\u001b[39m train_full_and_predict(LGBMRegressor, lgbm_best_params, X, y, X_test)\n",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m, in \u001b[0;36mcv_search\u001b[0;34m(model_name, model_ctor, param_grid, X, y, n_splits, random_state)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_tr, y_va \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[tr_idx], y\u001b[38;5;241m.\u001b[39miloc[va_idx]\n\u001b[1;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m model_ctor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_va)\n\u001b[1;32m     41\u001b[0m fold_rmses\u001b[38;5;241m.\u001b[39mappend(rmse(y_va, pred))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3654\u001b[0m     )\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 3656\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[1;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[1;32m   2587\u001b[0m             )\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 2590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:2123\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[1;32m   2121\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[0;32m-> 2123\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2130\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:868\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdf_dtypes)\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 868\u001b[0m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    869\u001b[0m     feature_name,\n\u001b[1;32m    870\u001b[0m     categorical_feature,\n\u001b[1;32m    871\u001b[0m     pandas_categorical,\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:814\u001b[0m, in \u001b[0;36m_pandas_to_numpy\u001b[0;34m(data, target_dtype)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pandas_to_numpy\u001b[39m(\n\u001b[1;32m    811\u001b[0m     data: pd_DataFrame,\n\u001b[1;32m    812\u001b[0m     target_dtype: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.typing.DTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 814\u001b[0m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/deeplearning/lib/python3.9/site-packages/lightgbm/basic.py:805\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[0;34m(pandas_dtypes_series)\u001b[0m\n\u001b[1;32m    799\u001b[0m bad_pandas_dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[1;32m    803\u001b[0m ]\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: TOWN: object, BLOCK: object, STREET: object"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CV + Hyperparam Search with tqdm, export CSV submissions\n",
    "# Models: RandomForest / XGBoost / LightGBM\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "TARGET_COL = \"RESALE_PRICE\"\n",
    "features = [c for c in df_train.columns if c != TARGET_COL]\n",
    "X = df_train[features]\n",
    "y = df_train[TARGET_COL]\n",
    "X_test = df_test[features]\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def cv_search(model_name, model_ctor, param_grid, X, y, n_splits=5, random_state=42):\n",
    "    best_params, best_rmse = None, float(\"inf\")\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    pbar = tqdm(grid, desc=f\"{model_name} grid\", leave=True)\n",
    "\n",
    "    for params in pbar:\n",
    "        fold_rmses = []\n",
    "        for tr_idx, va_idx in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            model = model_ctor(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            pred = model.predict(X_va)\n",
    "            fold_rmses.append(rmse(y_va, pred))\n",
    "\n",
    "        mean_rmse = float(np.mean(fold_rmses))\n",
    "        if mean_rmse < best_rmse:\n",
    "            best_rmse, best_params = mean_rmse, params\n",
    "        pbar.set_postfix({\"cv_rmse\": f\"{mean_rmse:.2f}\", \"best\": f\"{best_rmse:.2f}\"})\n",
    "\n",
    "    print(f\"{model_name} best params: {best_params}\")\n",
    "    print(f\"{model_name} best CV RMSE: {best_rmse:.2f}\")\n",
    "    return best_params, best_rmse\n",
    "\n",
    "def train_full_and_predict(model_ctor, params, X, y, X_test):\n",
    "    model = model_ctor(**params)\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "def save_submission(preds, path):\n",
    "    sub = pd.DataFrame({\"Id\": np.arange(len(preds)), \"Predicted\": preds})\n",
    "    sub.to_csv(path, index=False)\n",
    "    return sub\n",
    "\n",
    "\n",
    "# rf_grid = {\n",
    "#     \"n_estimators\": [300, 500],\n",
    "#     \"max_depth\": [12, 20, 30],\n",
    "#     \"min_samples_split\": [2, 5],\n",
    "#     \"min_samples_leaf\": [1, 2],\n",
    "#     \"random_state\": [42],\n",
    "#     \"n_jobs\": [-1],\n",
    "# }\n",
    "\n",
    "lgbm_grid = {\n",
    "    \"n_estimators\": [400, 700],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [-1, 8],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"reg_lambda\": [0.0, 1.0],\n",
    "    \"random_state\": [42],\n",
    "    \"n_jobs\": [-1],\n",
    "}\n",
    "\n",
    "\n",
    "# rf_best_params, rf_cv = cv_search(\"RandomForest\", RandomForestRegressor, rf_grid, X, y)\n",
    "\n",
    "lgbm_best_params, lgbm_cv = cv_search(\"LightGBM\", LGBMRegressor, lgbm_grid, X, y)\n",
    "\n",
    "\n",
    "# rf_preds   = train_full_and_predict(RandomForestRegressor, rf_best_params, X, y, X_test)\n",
    "\n",
    "lgbm_preds = train_full_and_predict(LGBMRegressor, lgbm_best_params, X, y, X_test)\n",
    "\n",
    "\n",
    "# save_submission(rf_preds,   \"rf_submission.csv\")\n",
    "\n",
    "save_submission(lgbm_preds, \"lgbm_submission.csv\")\n",
    "\n",
    "\n",
    "cv_scores = {\n",
    "    # \"RandomForest\": rf_cv,\n",
    "    \n",
    "    \"LightGBM\": lgbm_cv\n",
    "}\n",
    "best_name = min(cv_scores, key=cv_scores.get)\n",
    "print(f\"\\n Best by CV RMSE: {best_name} ({cv_scores[best_name]:.2f})\")\n",
    "\n",
    "best_preds = {\"LightGBM\": lgbm_preds}[best_name]\n",
    "save_submission(best_preds, \"submission_best.csv\")\n",
    "\n",
    "print(\"\\nFiles saved:\")\n",
    "# print(\" - rf_submission.csv\")\n",
    "print(\" - lgbm_submission.csv\")\n",
    "print(\" - submission_best.csv (best by CV)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
